{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Grammarly Check.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNPLRG0VzmRY7OU7KpDtbe6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WibuSOS/live-chat-translator/blob/ai-branch/Grammarly_Check.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goqVhCo_2LLC"
      },
      "source": [
        "# import unicodedata\n",
        "import re\n",
        "import math\n",
        "import random\n",
        "from random import shuffle\n",
        "from io import open\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "# import torch.cuda\n",
        "\n",
        "# this line clears sys to allow for argparse to work as gradient clipper\n",
        "import sys; sys.argv=['']; del sys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzdudN4jrkvF"
      },
      "source": [
        "# This function converts a Unicode string to plain ASCII \n",
        "# from https://stackoverflow.com/a/518232/2809427\n",
        "# def uniToAscii(sentence):\n",
        "#     return ''.join(\n",
        "#         c for c in unicodedata.normalize('NFD', sentence)\n",
        "#         if unicodedata.category(c) != 'Mn'\n",
        "#     )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters (from pytorch)\n",
        "def normalizeString(s):\n",
        "    s = re.sub(r\" ##AT##-##AT## \", r\" \", s)\n",
        "    s = s.lower().strip()\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s\n",
        "\n",
        "# Denote patterns that sentences must start with to be kept in dataset. \n",
        "# Can be changed if desired (from pytorch)\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s\",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")\n",
        "\n",
        "# Filters each input-output pair, keeping sentences that are less than max_length \n",
        "# if start_filter is true, also filters out sentences that don't start with eng_prefixes\n",
        "def filterPair(p, max_length, start_filter):\n",
        "    filtered = len(p[0].split(' ')) < max_length and \\\n",
        "        len(p[1].split(' ')) < max_length \n",
        "    if start_filter:\n",
        "        return filtered and p[1].startswith(eng_prefixes)\n",
        "    else:\n",
        "        return filtered\n",
        "\n",
        "# Filters all of the input-output language pairs in the dataset using filterPair \n",
        "# for each pair (from pytorch)\n",
        "def filterPairs(pairs, max_length, start_filter):\n",
        "    return [pair for pair in pairs if filterPair(pair, max_length, start_filter)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Iw0k3X67SVO"
      },
      "source": [
        "# start of sentence tag\n",
        "SOS_token = 0\n",
        "\n",
        "# end of sentence tag\n",
        "EOS_token = 1\n",
        "\n",
        "# unknown word tag (this is used to handle words that are not in our Vocabulary)\n",
        "UNK_token = 2\n",
        "\n",
        "# Lang class, used to store the vocabulary of each language\n",
        "class Lang:\n",
        "    def __init__(self, language):\n",
        "        self.language_name = language\n",
        "        self.word_to_index = {\"SOS\":SOS_token, \"EOS\":EOS_token, \"<UNK>\":UNK_token}\n",
        "        self.word_to_count = {}\n",
        "        self.index_to_word = {SOS_token: \"SOS\", EOS_token: \"EOS\", UNK_token: \"<UNK>\"}\n",
        "        self.vocab_size = 3\n",
        "        self.cutoff_point = -1\n",
        "\n",
        "    def countSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.countWords(word)\n",
        "\n",
        "    # counts the number of times each word appears in the dataset\n",
        "    def countWords(self, word):\n",
        "        if word not in self.word_to_count:\n",
        "            self.word_to_count[word] = 1\n",
        "        else:\n",
        "            self.word_to_count[word] += 1\n",
        "\n",
        "    # if the number of unique words in the dataset is larger than the\n",
        "    # specified max_vocab_size, creates a cutoff point that is used to\n",
        "    # leave infrequent words out of the vocabulary\n",
        "    def createCutoff(self, max_vocab_size):\n",
        "        word_freqs = list(self.word_to_count.values())\n",
        "        word_freqs.sort(reverse=True)\n",
        "        if len(word_freqs) > max_vocab_size:\n",
        "            self.cutoff_point = word_freqs[max_vocab_size]\n",
        "\n",
        "    # assigns each unique word in a sentence a unique index\n",
        "    def addSentence(self, sentence):\n",
        "        new_sentence = ''\n",
        "        for word in sentence.split(' '):\n",
        "            unk_word = self.addWord(word)\n",
        "            if not new_sentence:\n",
        "                new_sentence = unk_word\n",
        "            else:\n",
        "                new_sentence = new_sentence + ' ' + unk_word\n",
        "        return new_sentence\n",
        "\n",
        "    # assigns a word a unique index if not already in vocabulary\n",
        "    # and it appeaars often enough in the dataset\n",
        "    # (self.word_to_count is larger than self.cutoff_point)\n",
        "    def addWord(self, word):\n",
        "        if self.word_to_count[word] > self.cutoff_point:\n",
        "            if word not in self.word_to_index:\n",
        "                self.word_to_index[word] = self.vocab_size\n",
        "                self.index_to_word[self.vocab_size] = word\n",
        "                self.vocab_size += 1\n",
        "            return word\n",
        "        else:\n",
        "            return self.index_to_word[2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fygyn0W0uZ5I"
      },
      "source": [
        "# converts a sentence to one hot encoding vectors - pytorch allows us to just\n",
        "# use the number corresponding to the unique index for that word,\n",
        "# rather than a complete one hot encoding vector for each word\n",
        "\n",
        "def indexesFromSentence(lang, sentence):\n",
        "    indexes = []\n",
        "    for word in sentence.split(' '):\n",
        "        try:\n",
        "            indexes.append(lang.word_to_index[word])\n",
        "        except:\n",
        "            indexes.append(lang.word_to_index[\"<UNK>\"])\n",
        "    return indexes\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    result = torch.LongTensor(indexes).view(-1)\n",
        "    if use_cuda:\n",
        "        return result.cuda()\n",
        "    else:\n",
        "        return result\n",
        "\n",
        "# converts from tensor of one hot encoding vector indices to sentence\n",
        "# def sentenceFromTensor(lang, tensor):\n",
        "#     raw = tensor.data\n",
        "#     words = []\n",
        "#     for num in raw:\n",
        "#         words.append(lang.index_to_word[num.item()])\n",
        "#     return ' '.join(words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5rhs0JYoEUR"
      },
      "source": [
        "# prepares both the input and output Lang classes from the passed dataset\n",
        "def prepareLangs(file_path, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "    if len(file_path) == 2:\n",
        "        lang1_lines = open(file_path[0], encoding='utf-8').\\\n",
        "            read().strip().split('\\n')\n",
        "        lang2_lines = open(file_path[1], encoding='utf-8').\\\n",
        "            read().strip().split('\\n')\n",
        "        if len(lang1_lines) != len(lang2_lines):\n",
        "            print(\"Input and output text sizes do not align\")\n",
        "            print(\"Number of lang1 lines: %s \" %len(lang1_lines))\n",
        "            print(\"Number of lang2 lines: %s \" %len(lang2_lines))\n",
        "            quit()\n",
        "        pairs = []\n",
        "        for line in range(len(lang1_lines)):\n",
        "            pairs.append([normalizeString(lang1_lines[line]),\n",
        "                          normalizeString(lang2_lines[line])])\n",
        "        print(\"pairs=%s with file_path=%s has been created \" % (len(pairs), len(file_path)))\n",
        "    elif len(file_path) == 1:\n",
        "        lines = open(file_path[0], encoding='utf-8').\\\n",
        "    \tread().strip().split('\\n')\n",
        "        pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "        print(\"pairs=%s with file_path=%s has been created \" % (len(pairs), len(file_path)))\n",
        "    print(lines[0])\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        print(\"pair reverse has been created\")\n",
        "    else:\n",
        "        print(\"pair reverse is not created\")\n",
        "    \n",
        "    return pairs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyH57K0AJPCW"
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self,input_size,hidden_size,layers=1,dropout=0.1,\n",
        "                bidirectional=True):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "\n",
        "        if bidirectional:\n",
        "            self.directions = 2\n",
        "        else:\n",
        "            self.directions = 1\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = layers\n",
        "        self.dropout = dropout\n",
        "        self.embedder = nn.Embedding(input_size,hidden_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.lstm = nn.LSTM(input_size=hidden_size,hidden_size=hidden_size,\n",
        "                        num_layers=layers,dropout=dropout,\n",
        "                        bidirectional=bidirectional,batch_first=False)\n",
        "        self.fc = nn.Linear(hidden_size*self.directions, hidden_size)\n",
        "\n",
        "    def forward(self, input_data, h_hidden, c_hidden):\n",
        "        embedded_data = self.embedder(input_data)\n",
        "        embedded_data = self.dropout(embedded_data)\n",
        "        hiddens, outputs = self.lstm(embedded_data, (h_hidden, c_hidden))\n",
        "\n",
        "        return hiddens, outputs\n",
        "\n",
        "    # creates initial hidden states for encoder corresponding to batch size\n",
        "    def create_init_hiddens(self, batch_size):\n",
        "        h_hidden = Variable(torch.zeros(self.num_layers*self.directions, \n",
        "                                    batch_size, self.hidden_size))\n",
        "        c_hidden = Variable(torch.zeros(self.num_layers*self.directions, \n",
        "                                    batch_size, self.hidden_size))\n",
        "        \n",
        "        return h_hidden, c_hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0QThJ7mJYRo"
      },
      "source": [
        "class DecoderAttn(nn.Module):\n",
        "\tdef __init__(self, hidden_size, output_size, layers=1, dropout=0.1, bidirectional=True):\n",
        "\t\tsuper(DecoderAttn, self).__init__()\n",
        "\n",
        "\t\tif bidirectional:\n",
        "\t\t\tself.directions = 2\n",
        "\t\telse:\n",
        "\t\t\tself.directions = 1\n",
        "\t\tself.output_size = output_size\n",
        "\t\tself.hidden_size = hidden_size\n",
        "\t\tself.num_layers = layers\n",
        "\t\tself.dropout = dropout\n",
        "\t\tself.embedder = nn.Embedding(output_size,hidden_size)\n",
        "\t\tself.dropout = nn.Dropout(dropout)\n",
        "\t\tself.score_learner = nn.Linear(hidden_size*self.directions, \n",
        "                                   hidden_size*self.directions)\n",
        "\t\tself.lstm = nn.LSTM(input_size=hidden_size,hidden_size=hidden_size,\n",
        "                        num_layers=layers,dropout=dropout,\n",
        "                        bidirectional=bidirectional,batch_first=False)\n",
        "\t\tself.context_combiner = nn.Linear((hidden_size*self.directions)\n",
        "                                      +(hidden_size*self.directions), hidden_size)\n",
        "\t\tself.tanh = nn.Tanh()\n",
        "\t\tself.output = nn.Linear(hidden_size, output_size)\n",
        "\t\tself.soft = nn.Softmax(dim=1)\n",
        "\t\tself.log_soft = nn.LogSoftmax(dim=1)\n",
        "\n",
        "\tdef forward(self, input_data, h_hidden, c_hidden, encoder_hiddens):\n",
        "\t\tembedded_data = self.embedder(input_data)\n",
        "\t\tembedded_data = self.dropout(embedded_data)\t\n",
        "\t\tbatch_size = embedded_data.shape[1]\n",
        "\t\thiddens, outputs = self.lstm(embedded_data, (h_hidden, c_hidden))\t\n",
        "\t\ttop_hidden = outputs[0].view(self.num_layers,self.directions,\n",
        "                                 hiddens.shape[1],\n",
        "                                 self.hidden_size)[self.num_layers-1]\n",
        "\t\ttop_hidden = top_hidden.permute(1,2,0).contiguous().view(batch_size,-1,1)\n",
        "\n",
        "\t\tprep_scores = self.score_learner(encoder_hiddens.permute(1,0,2))\n",
        "\t\tscores = torch.bmm(prep_scores, top_hidden)\n",
        "\t\tattn_scores = self.soft(scores)\n",
        "\t\tcon_mat = torch.bmm(encoder_hiddens.permute(1,2,0),attn_scores)\n",
        "\t\th_tilde = self.tanh(self.context_combiner(torch.cat((con_mat,\n",
        "                                                         top_hidden),dim=1)\n",
        "                                              .view(batch_size,-1)))\n",
        "\t\tpred = self.output(h_tilde)\n",
        "\t\tpred = self.log_soft(pred)\n",
        "\n",
        "\t\treturn pred, outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9XBtIEnK0Fu"
      },
      "source": [
        "# Returns the predicted translation of a given input sentence. Predicted\n",
        "# translation is trimmed to length of cutoff_length argument\n",
        "\n",
        "def evaluate(encoder, decoder, input_lang, output_lang, sentence, cutoff_length):\n",
        "    with torch.no_grad():\n",
        "        input_variable = tensorFromSentence(input_lang, sentence)\n",
        "        input_variable = input_variable.view(-1,1)\n",
        "\n",
        "        enc_h_hidden, enc_c_hidden = encoder.create_init_hiddens(1)\n",
        "        enc_hiddens, enc_outputs = encoder(input_variable, enc_h_hidden, enc_c_hidden)\n",
        "\n",
        "        decoder_input = Variable(torch.LongTensor(1,1).fill_(output_lang.word_to_index.get(\"SOS\")).cuda()) if use_cuda \\\n",
        "                        else Variable(torch.LongTensor(1,1).fill_(output_lang.word_to_index.get(\"SOS\")))\n",
        "        dec_h_hidden = enc_outputs[0]\n",
        "        dec_c_hidden = enc_outputs[1]\n",
        "\n",
        "        decoded_words = []\n",
        "\n",
        "        for di in range(cutoff_length):\n",
        "            pred, dec_outputs = decoder(decoder_input, dec_h_hidden, dec_c_hidden, enc_hiddens)\n",
        "            topv, topi = pred.topk(1,dim=1)\n",
        "            ni = topi.item()\n",
        "\n",
        "            if ni == output_lang.word_to_index.get(\"EOS\"):\n",
        "                # decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index_to_word[ni])\n",
        "\n",
        "            decoder_input = Variable(torch.LongTensor(1,1).fill_(ni).cuda()) if use_cuda \\\n",
        "                            else Variable(torch.LongTensor(1,1).fill_(ni))\n",
        "            dec_h_hidden = dec_outputs[0]\n",
        "            dec_c_hidden = dec_outputs[1]\n",
        "\n",
        "        output_sentence = ' '.join(decoded_words)\n",
        "\n",
        "        return output_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMriMtn6NqdU"
      },
      "source": [
        "# HYPERPARAMETERS: FEEL FREE TO PLAY WITH THESE TO TRY TO ACHIEVE BETTER RESULTS\n",
        "\n",
        "# signifies whether the Encoder and Decoder should be bidirectional LSTMs or not\n",
        "bidirectional = True\n",
        "if bidirectional:\n",
        "\tdirections = 2\n",
        "else:\n",
        "\tdirections = 1\n",
        "\n",
        "# number of layers in both the Encoder and Decoder\n",
        "layers = 2\n",
        "\n",
        "# Hidden size of the Encoder and Decoder\n",
        "hidden_size = 600\n",
        "\n",
        "# Dropout value for Encoder and Decoder\n",
        "dropout = 0.8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlJRMisKNKR7"
      },
      "source": [
        "# LOAD CONFIGURATIONS\n",
        "\n",
        "# Set the common name of the loading files\n",
        "common_file_name = \"testdata.tatoeba_trim.20_vocab.25000_directions.2_layers.4_hidden.100_dropout.0.5_learningrate.1_batch.10_epochs.100\"\n",
        "id_lang = 'id'\n",
        "en_lang = 'en'\n",
        "dataset = 'tatoeba'\n",
        "directory = ''\n",
        "\n",
        "# Set these configurations if you want to load models from google drive\n",
        "load_from_drive = True\n",
        "if load_from_drive:\n",
        "    common_file_name = \"testdata>tatoeba_trim>20_vocab>25000_directions>2_layers>4_hidden>100_dropout>0.5_learningrate>1_batch>10_epochs>100\"\n",
        "    super_directory = '/content/drive/'\n",
        "    experiment_directory = 'MyDrive/Kuliah/Thesis/experiment/grammarly/tatoeba/'\n",
        "    directory = super_directory + experiment_directory\n",
        "\n",
        "# denotes the fixedness of the randomness\n",
        "# applied to torch.random and python random module\n",
        "seed_value = 10\n",
        "\n",
        "# file path of dataset in the form of a list. If translated sentences are\n",
        "# stored in two files, this list will have two elements\n",
        "raw_data_file_path = [directory+'tatoeba_id_en.txt']\n",
        "\n",
        "# True if you want to reverse the order of the sentence pairs. For example, \n",
        "# in our dataset the sentence pairs list the English sentence first followed by\n",
        "# the French translation. But we want to translate from French to English,\n",
        "# so we set reverse as True.\n",
        "reverse = False\n",
        "\n",
        "# Remove sentences from dataset that are longer than trim (in either language)\n",
        "trim = 30\n",
        "\n",
        "# max number of words in the vocabulary for both languages\n",
        "max_vocab_size = 30000\n",
        "\n",
        "# if true removes sentences from the dataset that don't start with eng_prefixes.\n",
        "# Typically will want to use False, but implemented to compare results with Pytorch\n",
        "# tutorial. Can also change the eng_prefixes to prefixes of other languages or\n",
        "# other English prefixes. Just be sure that the prefixes apply to the OUTPUT\n",
        "# language (i.e. the language that the model is translating to NOT from)\n",
        "start_filter = False\n",
        "\n",
        "# denotes what percentage of the data to use as training data. the remaining \n",
        "# percentage becomes dev/validation data and test data.\n",
        "# Typically want to use 0.8-0.9.\n",
        "perc_train_set = 0.8\n",
        "\n",
        "# Number of sentences that are put into Grammarly check.\n",
        "# taken from test set.\n",
        "checkup_size = 100\n",
        "\n",
        "# Set the name of the loading files\n",
        "id_vocab_file = directory + id_lang + '_4310_' + dataset + '_vocab.p'\n",
        "en_vocab_file = directory + en_lang + '_3572_' + dataset + '_vocab.p'\n",
        "id_en_enc_file = '%s%s_%s_enc_direction_%s_layer_%s_hidden_%s_dropout_%s.pth' % (directory, id_lang, en_lang, directions, layers, hidden_size, dropout)\n",
        "id_en_dec_file = '%s%s_%s_dec_direction_%s_layer_%s_hidden_%s_dropout_%s.pth' % (directory, id_lang, en_lang, directions, layers, hidden_size, dropout)\n",
        "en_id_enc_file = '%s%s_%s_enc_direction_%s_layer_%s_hidden_%s_dropout_%s.pth' % (directory, en_lang, id_lang, directions, layers, hidden_size, dropout)\n",
        "en_id_dec_file = '%s%s_%s_dec_direction_%s_layer_%s_hidden_%s_dropout_%s.pth' % (directory, en_lang, id_lang, directions, layers, hidden_size, dropout)\n",
        "\n",
        "# File path to predicted sentences for Grammarly test\n",
        "print_to = directory + 'grammarly_test' + '.txt'\n",
        "\n",
        "# Mandatory variables initialization\n",
        "device = torch.device('cpu')\n",
        "# use_cuda = torch.cuda.is_available()\n",
        "use_cuda = False\n",
        "id_vocab = None\n",
        "en_vocab = None\n",
        "id_en_encoder = None\n",
        "id_en_decoder = None\n",
        "en_id_encoder = None\n",
        "en_id_decoder = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "id4SDcVSgBcd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67de3884-be51-4a85-ef72-39ad6e47b906"
      },
      "source": [
        "# LOAD FROM DRIVE: OPTIONAL!!!\n",
        "# execute this cell if you want to load models from google drive\n",
        "\n",
        "if load_from_drive:\n",
        "    from google.colab import drive\n",
        "    drive.mount(super_directory)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFkPTTP664QY"
      },
      "source": [
        "# from google.colab import files\n",
        "# uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsdxuTgiLzVV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2274d794-8d91-4645-df36-e90b5c7bb2f6"
      },
      "source": [
        "# LOAD EVERYTHING\n",
        "\n",
        "# sets the fixedness of the randomness\n",
        "torch.random.manual_seed(seed_value)\n",
        "random.seed(seed_value)\n",
        "\n",
        "pairs = prepareLangs(raw_data_file_path, reverse=reverse)\n",
        "\n",
        "id_vocab = pickle.load(open(id_vocab_file,'rb'))\n",
        "en_vocab = pickle.load(open(en_vocab_file,'rb'))\n",
        "\n",
        "id_en_encoder = EncoderRNN(id_vocab.vocab_size, hidden_size, layers=layers, \n",
        "                           dropout=dropout, bidirectional=bidirectional)\n",
        "id_en_decoder = DecoderAttn(hidden_size, en_vocab.vocab_size, layers=layers, \n",
        "                            dropout=dropout, bidirectional=bidirectional)\n",
        "\n",
        "id_en_encoder.load_state_dict(torch.load(id_en_enc_file, map_location=device))\n",
        "id_en_decoder.load_state_dict(torch.load(id_en_dec_file, map_location=device))\n",
        "\n",
        "id_en_encoder.eval()\n",
        "id_en_decoder.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "pairs=7141 with file_path=1 has been created \n",
            "Lari!\tRun!\n",
            "pair reverse is not created\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecoderAttn(\n",
              "  (embedder): Embedding(3572, 600)\n",
              "  (dropout): Dropout(p=0.8, inplace=False)\n",
              "  (score_learner): Linear(in_features=1200, out_features=1200, bias=True)\n",
              "  (lstm): LSTM(600, 600, num_layers=2, dropout=0.8, bidirectional=True)\n",
              "  (context_combiner): Linear(in_features=2400, out_features=600, bias=True)\n",
              "  (tanh): Tanh()\n",
              "  (output): Linear(in_features=600, out_features=3572, bias=True)\n",
              "  (soft): Softmax(dim=1)\n",
              "  (log_soft): LogSoftmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wvlFpDFMXVv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94590dbf-16eb-41dd-dd5c-8810696bc320"
      },
      "source": [
        "if trim != 0:\n",
        "    pairs = filterPairs(pairs, trim, start_filter)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "\n",
        "shuffle(pairs)\n",
        "\n",
        "temp_pairs = pairs[:math.ceil(perc_train_set*len(pairs))]\n",
        "test_pairs = pairs[math.ceil(perc_train_set*len(pairs)):]\n",
        "train_pairs = temp_pairs[:math.ceil(perc_train_set*len(temp_pairs))]\n",
        "dev_pairs = temp_pairs[math.ceil(perc_train_set*len(temp_pairs)):]\n",
        "\n",
        "# Test a sentence outside the dataset\n",
        "for index in range(checkup_size):\n",
        "    prediction = evaluate(id_en_encoder, id_en_decoder, id_vocab, en_vocab, normalizeString(test_pairs[index][0]), cutoff_length=trim)\n",
        "    print(index+1, prediction)\n",
        "    with open(print_to, 'a+') as f:\n",
        "        f.write(prediction + '\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trimmed to 7140 sentence pairs\n",
            "1 i am her for a . .\n",
            "2 tom looks confused .\n",
            "3 he was a of his .\n",
            "4 for children to to years on the jam can can be to to . .\n",
            "5 the the the the the the the . .\n",
            "6 how this is this ? ?\n",
            "7 i m sorry i didn t all you all . .\n",
            "8 i m tired .\n",
            "9 i think a m very . .\n",
            "10 do you want to wait of us us you ? ?\n",
            "11 we had a lot of our we we had .\n",
            "12 tom will be in the .\n",
            "13 i m sorry i i i m t . .\n",
            "14 when will you leave home ?\n",
            "15 i have a . .\n",
            "16 tom is reading a book .\n",
            "17 that is really true .\n",
            "18 tom is still one one nothing who .\n",
            "19 don t be too much .\n",
            "20 when will you leave home ?\n",
            "21 tom has to .\n",
            "22 what do you want to do ?\n",
            "23 i know it s understand is all .\n",
            "24 tom left to get hour ago .\n",
            "25 please turn the the . .\n",
            "26 tom looks confused .\n",
            "27 let s do it .\n",
            "28 is there a highest on in ? ?\n",
            "29 the island is a is .\n",
            "30 i am tired of that .\n",
            "31 tom went to the school .\n",
            "32 this is the first time we had a had in in boston .\n",
            "33 tom has three three children .\n",
            "34 i ve just finished breakfast .\n",
            "35 i m pretty impressed with tom s .\n",
            "36 the island is like of a children . .\n",
            "37 the have is . .\n",
            "38 what re you doing ?\n",
            "39 wake !\n",
            "40 i don t want to say that that .\n",
            "41 tom works every day day .\n",
            "42 it !\n",
            "43 the rain lasted two ten .\n",
            "44 what s you doing ?\n",
            "45 the meeting lasted to . .\n",
            "46 i haven t have lunch yet .\n",
            "47 you have to be good person . .\n",
            "48 it s what s s tom .\n",
            "49 my are still not .\n",
            "50 i m not like him . .\n",
            "51 he s a book a .\n",
            "52 your s very . .\n",
            "53 i don t know you you re . .\n",
            "54 let me take a look .\n",
            "55 i have to read the book that .\n",
            "56 this is my .\n",
            "57 he can t do this this of of let is a . . .\n",
            "58 i ve been two a . .\n",
            "59 tom walked at the chair .\n",
            "60 don t let me me . .\n",
            "61 i m not kidding .\n",
            "62 this is the only i .\n",
            "63 the local the museum is and thirty . .\n",
            "64 tom did what what he is mary with . .\n",
            "65 we know the say you don t make make .\n",
            "66 i m here here in this morning .\n",
            "67 where will we going ?\n",
            "68 we ll going to\n",
            "69 i will be be years next next next .\n",
            "70 tom is reading now .\n",
            "71 tom is on a street .\n",
            "72 there is a rare in our our here .\n",
            "73 don t make me .\n",
            "74 what what you want to do ?\n",
            "75 my your is . .\n",
            "76 i can t go tom .\n",
            "77 he wanted to be a . .\n",
            "78 i m a . .\n",
            "79 he s a beauty . .\n",
            "80 have you eaten dinner ?\n",
            "81 tom is in there the car in the the . .\n",
            "82 he is still in the . .\n",
            "83 i know to know to know this is ? ?\n",
            "84 i m glad happy you be it .\n",
            "85 i m to tell tom why i didn t do that that .\n",
            "86 my s all to . .\n",
            "87 may i use this this ?\n",
            "88 did you like your ? ?\n",
            "89 let s do it again again .\n",
            "90 may i use your passport ?\n",
            "91 tom looks as he he s a . .\n",
            "92 what will this first time ?\n",
            "93 tom is the only one who didn t love the . .\n",
            "94 i d to know it with .\n",
            "95 tom is a good .\n",
            "96 when was the last time you you the ? ?\n",
            "97 tom is playing day for a .\n",
            "98 i like with with my . .\n",
            "99 i know what tom is here .\n",
            "100 i was only of who what who to to tom .\n"
          ]
        }
      ]
    }
  ]
}